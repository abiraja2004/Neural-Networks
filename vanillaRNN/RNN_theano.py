#coding=utf8
"""
@author: lebron.ran
@file: RNN_theano.py
@time: 2017/3/2 0002-21:55
"""
import numpy as np
import theano
import theano.tensor as T
import operator

class RNN_theano:

    def __init__(self,word_dim,hidden_dim,bptt_truncate=4):

        self.word_dim = word_dim
        self.hidden_dim = hidden_dim
        self.bptt_truncate = bptt_truncate

        U = np.random.uniform(-np.sqrt(1./word_dim),np.sqrt(1./word_dim),(hidden_dim,word_dim))
        V = np.random.uniform(-np.sqrt(1./hidden_dim),np.sqrt(1./hidden_dim),(word_dim,hidden_dim))
        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim),(hidden_dim,hidden_dim))

        self.U = theano.shared(name='U',value=U.astype(theano.config.floatX))
        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))
        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))

        self.theano = {}
        self.__theano_build__()

    def __theano_build__(self):
        U,V,W = self.U,self.V,self.W
        x = T.ivector('x')
        y = T.ivector('y')

        def forward_propagation_step(x_t,s_t_prev,U,V,W):
            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))
            o_t = T.nnet.softmax(V.dot(s_t))

            return [o_t[0] ,s_t]
        [o,s],updates = theano.scan(
                                   forward_propagation_step,
                                   sequences=x,
                                   outputs_info=[None,dict(initial=T.zeros(self.hidden_dim))],
                                   non_sequences=[U,V,W],
                                   truncate_gradient=self.bptt_truncate,
                                   strict=True
                                   )

        prediction = T.argmax(o,axis=1)
        o_error = T.sum(T.nnet.categorical_crossentropy(o,y))

        #gradients
        dU = T.grad(o_error,U)
        dV = T.grad(o_error,V)
        dW = T.grad(o_error,W)

        #assgin function
        self.forward_propagation_step = theano.function([x],o)
        self.predict = theano.function([x],prediction)
        self.cost = theano.function([x,y],o_error)
        self.bptt = theano.function([x,y],[dU,dV,dW])

        #SGD
        learning_rate = T.scalar('learning_rate')
        self.sgd_step = theano.function([x,y,learning_rate],[],updates=[(self.U,self.U - learning_rate*dU),
                                                                        (self.V, self.V - learning_rate * dV),
                                                                        (self.W, self.W - learning_rate * dW)])

    def calculate_total_loss(self, X, Y):
        return np.sum([self.cost(x, y) for x, y in zip(X, Y)])

    def calculate_loss(self, X, Y):
        # Divide calculate_loss by the number of words
        num_words = np.sum([len(y) for y in Y])
        return self.calculate_total_loss(X, Y) / float(num_words)





    def gradient_check(self,x,y,h=0.001,toler=0.01):

        # get the result generated by our implement of BPTT,all we wnat to do here
        # is to check if it is correct
        bptt_gradients = self.bptt(x,y)

        #
        model_parameters = ['U','V','W']
        # perform gradient check for each parameters

        for pidx,pname in enumerate(model_parameters):

            # get the actual parameter value from the mode ,e.g.model.W
            parameter_T = operator.attrgetter(pname)(self)
            parameter = parameter_T.get_value()

            print 'Perform Gradient Check for Parameter %s with size %d.' % (pname,\
                                                            np.prod(parameter.shape))

            # iterate over element of parameter matrix e.g (0,0),(0,1),...
            it = np.nditer(parameter,flags=['multi_index'],op_flags=['readwrite'])
            while not it.finished:
                ix = it.multi_index

                # copy the original value
                original_value = parameter[ix]
                parameter[ix] = original_value + h
                parameter_T.set_value(parameter)

                gradplus = self.calculate_total_loss([x],[y])

                parameter[ix] = original_value - h

                parameter_T.set_value(parameter)

                gradminus = self.calculate_total_loss([x],[y])

                estimated_gradient = (gradplus - gradminus) / (2 * h)

                backprop_grad = bptt_gradients[pidx][ix]

                # calculate the relative error
                relative_error = np.abs(estimated_gradient - backprop_grad) / \
                                 (np.abs(estimated_gradient) + np.abs(backprop_grad))

                # reset the value
                parameter[ix] = original_value
                parameter_T.set_value(parameter)

                if relative_error > toler:
                    # the naive implement of bptt fails to calculate the gradients
                    print 'Gradient Check Error:parameter=%s,index = %s' % (pname,ix)
                    print '+h Loss:%f'%gradplus
                    print '-h Loss:%f' % gradminus
                    print 'Estimated Gradients is %f.' % estimated_gradient
                    print 'Back propagation Gradient is %f. ' % backprop_grad
                    print 'Relative Error is %f.' % relative_error

                it.iternext()
            print 'Gradient check for parameters %s passed' % pname

