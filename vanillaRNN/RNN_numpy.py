#coding=utf8
"""
@author: Lebron.Ran
@file: RNN_numpy.py
@time: 2017/2/28 0028-16:32

"""
import numpy as np
import operator

class RNN_numpy:
    def __init__(self,word_dim,hidden_dim=100,bptt_truncate=4):

        # assign variable
        self.word_dim = word_dim
        self.hidden_dim = hidden_dim
        self.bptt_truncate = bptt_truncate

        # randomly initial the networks parameters
        self.U = np.random.uniform(-np.sqrt(1.0/word_dim),np.sqrt(1.0/word_dim),(hidden_dim,word_dim))
        self.V = np.random.uniform(-np.sqrt(1.0/hidden_dim),np.sqrt(1.0/hidden_dim),(word_dim,hidden_dim))
        self.W = np.random.uniform(-np.sqrt(1.0/hidden_dim),np.sqrt(1.0/hidden_dim),(hidden_dim,hidden_dim))

    def forward_propagation(self,x):

        # the total number of time steps
        T = len(x)

        # during the process of forward propagation,we need to save the Ss to avoid calculate them later
        # also,we add an additional element for the initial hidden layer ,which we set to 0s.
        s = np.zeros((T+1,self.hidden_dim))
        s[-1] = np.zeros(self.hidden_dim)

        # the output for each time step ,and again we save them for later.
        y = np.zeros((T,self.word_dim))

        #during each time step..
        for t in range(T):

            # this is a simple expression of UX + Ws_t,because X is just a one-hot vector
            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))
            y[t] = self.softmax(self.V.dot(s[t]))

        return [y,s]

    def predict(self,x):

        # perform the forward propagation and return the index of highest score
        y,s = self.forward_propagation(x)
        return np.argmax(y,axis=1)

    def calculate_total_loss(self,x,y):
        L = 0
        # for each sentence
        for i in np.arange(len(y)):
            y_hat,s = self.forward_propagation(x[i])

            #calculate the loss of current sentence
            loss = y_hat[np.arange(len(y[i])),y[i]]
            L += -1 * np.sum(np.log(loss))

        return L
    def calculate_loss(self,x,y):
        N = np.sum([len(yi) for yi in y])
        return self.calculate_total_loss(x,y)*1.0 / N

    # a naive implement of Back Propagation Through Time algorithm
    def bptt(self,x,y):

        # time steps
        T = len(y)
        # forward propagation
        y_hat,s = self.forward_propagation(x)

        # initialize the differentiation of every variables
        dLdU = np.zeros(self.U.shape)
        dLdV = np.zeros(self.V.shape)
        dLdW = np.zeros(self.W.shape)

        # 求对z3 = V*S_3的偏导数
        delta_vs3 = y_hat
        delta_vs3[np.arange(len(y)),y] -= 1

        # for each output backwards
        for t in np.arange(T)[::-1]:

            # Error 对于 V 的偏导 仅与 当前 time step有关
            dLdV += np.outer(delta_vs3[t],s[t].T)

            delta_zi = self.V.T.dot(delta_vs3[t]) * (1 - (s[t] ** 2))

            # Back propagation Through time (for at most self.bptt_truncate)
            for bptt_step in np.arange(max(0,t - self.bptt_truncate),t+1)[::-1]:

                #print
                dLdW += np.outer(delta_zi,s[bptt_step - 1])
                dLdU[:,x[bptt_step]] += delta_zi

                # update the delta for next step
                delta_zi = self.W.T.dot(delta_zi) * (1 - (s[bptt_step - 1] ** 2))

        return [dLdU,dLdV,dLdW]


    def gradient_check(self,x,y,h=0.001,toler=0.01):

        # get the result generated by our implement of BPTT,all we wnat to do here
        # is to check if it is correct
        bptt_gradients = self.bptt(x,y)

        #
        model_parameters = ['U','V','W']
        # perform gradient check for each parameters

        for pidx,pname in enumerate(model_parameters):

            # get the actual parameter value from the mode ,e.g.model.W
            parameter = operator.attrgetter(pname)(self)

            print 'Perform Gradient Check for Parameter %s with size %d.' % (pname,\
                                                            np.prod(parameter.shape))

            # iterate over element of parameter matrix e.g (0,0),(0,1),...
            it = np.nditer(parameter,flags=['multi_index'],op_flags=['readwrite'])
            while not it.finished:
                ix = it.multi_index

                # copy the original value
                original_value = parameter[ix]
                parameter[ix] = original_value + h
                gradplus = self.calculate_total_loss([x],[y])
                parameter[ix] = original_value - h
                gradminus = self.calculate_total_loss([x],[y])
                estimated_gradient = (gradplus - gradminus) / 2 * h

                backprop_grad = bptt_gradients[pidx][ix]

                # calculate the relative error
                relative_error = np.abs(estimated_gradient - backprop_grad) / \
                                 (np.abs(estimated_gradient) + np.abs(backprop_grad))

                # reset the value
                parameter[ix] = original_value
                if relative_error > toler:
                    # the naive implement of bptt fails to calculate the gradients
                    print 'Gradient Check Error:parameter=%s,index = %s' % (pname,ix)
                    print '+h Loss:%f'%gradplus
                    print '-h Loss:%f' % gradminus
                    print 'Estimated Gradients is %f.' % estimated_gradient
                    print 'Back propagation Gradient is %f. ' % backprop_grad
                    print 'Relative Error is %f.' % relative_error

                it.iternext()
            print 'Gradient check for parameters %s passed' % pname







    # helper function to calculate softmax function
    def softmax(self,x):
        xt = np.exp(x)
        return xt / np.sum(xt)

